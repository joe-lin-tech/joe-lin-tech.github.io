---
---

@string{aps = {American Physical Society,}}

@misc{he2015deepresiduallearningimage,
    title = {Deep Residual Learning for Image Recognition}, 
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year = {2015},
    eprint = {1512.03385},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/1512.03385}, 
}

@misc{dosovitskiy2021imageworth16x16words,
    title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
    author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    year = {2021},
    eprint = {2010.11929},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2010.11929}, 
}

@misc{cheng2023sammed2d,
    title = {SAM-Med2D}, 
    author = {Junlong Cheng and Jin Ye and Zhongying Deng and Jianpin Chen and Tianbin Li and Haoyu Wang and Yanzhou Su and Ziyan Huang and Jilong Chen and Lei Jiang and Hui Sun and Junjun He and Shaoting Zhang and Min Zhu and Yu Qiao},
    year = {2023},
    eprint = {2308.16184},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2308.16184}, 
}

@article {Avesta2022.11.03.22281923,
	author = {Avesta, Arman and Hossain, Sajid and Lin, MingDe and Aboian, Mariam and Krumholz, Harlan M. and Aneja, Sanjay},
	title = {Comparing 3D, 2.5D, and 2D Approaches to Brain Image Segmentation},
	elocation-id = {2022.11.03.22281923},
	year = {2022},
	doi = {10.1101/2022.11.03.22281923},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Deep-learning methods for auto-segmenting brain images either segment one slice of the image (2D), five consecutive slices of the image (2.5D), or an entire volume of the image (3D). Whether one approach is superior for auto-segmenting brain images is not known.We compared these three approaches (3D, 2.5D, and 2D) across three auto-segmentation models (capsule networks, UNets, and nnUNets) to segment brain structures. We used 3430 brain MRIs, acquired in a multi-institutional study, to train and test our models. We used the following performance metrics: segmentation accuracy, performance with limited training data, required computational memory, and computational speed during training and deployment.3D, 2.5D, and 2D approaches respectively gave the highest to lowest Dice scores across all models. 3D models maintained higher Dice scores when the training set size was decreased from 3199 MRIs down to 60 MRIs. 3D models converged 20\% to 40\% faster during training and were 30\% to 50\% faster during deployment. However, 3D models require 20 times more computational memory compared to 2.5D or 2D models.This study showed that 3D models are more accurate, maintain better performance with limited training data, and are faster to train and deploy. However, 3D models require more computational memory compared to 2.5D or 2D models.Competing Interest StatementThe authors have declared no competing interest.Funding StatementArman Avesta is a PhD Student in the Investigative Medicine Program at Yale which is supported by CTSA Grant Number UL1 TR001863 from the National Center for Advancing Translational Science a component of the National Institutes of Health (NIH). This work was also supported by the Radiological Society of North America (RSNA) Fellow Research Grant Number RF2212. The contents of this article are solely the responsibility of the authors and do not necessarily represent the official views of NIH or RSNA.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:the data used in this study were obtained from the Alzheimer Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). We obtained T1-weighted MRIs of 3430 patients in the Alzheimer Disease Neuroimaging Initiative study from this data-sharing platform. The investigators within the ADNI contributed to the design and implementation of ADNI but did not participate in the analysis or writing of this article.I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.Yesthe data used in this study were obtained from the Alzheimer Disease Neuroimaging Initiative (ADNI) database available at: adni.loni.usc.edu. https://adni.loni.usc.edu/ 2D segmentationtwo-dimensional segmentation2.5D segmentationenhanced two-dimensional segmentation3D segmentationthree-dimensional segmentationADNIAlzheimer{\textquoteright}s disease neuroimaging initiativeCapsNetcapsule networkCPUcentral processing unitCTcomputed tomographyGBgiga-byteGPUgraphics processing unitMRImagnetic resonance imaging},
	URL = {https://www.medrxiv.org/content/early/2022/11/21/2022.11.03.22281923},
	eprint = {https://www.medrxiv.org/content/early/2022/11/21/2022.11.03.22281923.full.pdf},
	journal = {medRxiv}
}
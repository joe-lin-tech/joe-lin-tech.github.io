<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> abdominal trauma detection | Joe Lin </title> <meta name="author" content="Joe Lin"> <meta name="description" content="rsna kaggle competition"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joe-lin-tech.github.io/projects/rsna_atd/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joe</span> Lin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">abdominal trauma detection</h1> <p class="post-description">rsna kaggle competition</p> </header> <article> <blockquote> <p>Computed tomography (CT) scans have become crucial for patient evaluation when it comes to injury detection, but interpreting this data can be complex and time-consuming. So, the <a href="https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection" rel="external nofollow noopener" target="_blank">RSNA Abdominal Trauma Detection</a> Kaggle competition challenged the community to devise a deep learning approach to classify abdominal injuries from multi-phase CT scans in the hopes of assisting medical professionals with diagnosis.</p> </blockquote> <h2 id="overview">Overview</h2> <p>In this competition, we’re given computed tomography (CT) scans provided by various institutions. The goal is to build a model that can extract critical features within these scans and classify organ injuries (if present) at the liver, spleen, and kidney, as well as any bowel and extravasation injuries.</p> <h2 id="pipeline--model-architecture">Pipeline + Model Architecture</h2> <p>In this project, multiple prominent architectures were pipelined together to form several solutions. The major pipelines experimented on within this repository are summarized as follows:</p> <ul> <li>2.5D Backbone Feature Extractor → 3D CNN → Prediction Head</li> <li>Mask Generator → Merge Input and Mask → 3D CNN → Prediction Head</li> <li>Slice Predictor → Input Slice Interpolation → 2.5D Backbone Feature Extractor → 3D CNN → Prediction Head</li> <li>Mask Generator → Backbone Feature Extractor (one for input and one for mask) → Merge Input and Mask Features → 3D CNN → Prediction Head</li> </ul> <h3 id="backbone-feature-extractor">Backbone Feature Extractor</h3> <p>The primary backbone feature extractors utilized were ResNet and Vision Transformer. These architectures are notable for their ability to effectively extract features from visual data through the use of residual connections and self-attention modules <a class="citation" href="#he2015deepresiduallearningimage">(He et al., 2015)</a>, <a class="citation" href="#dosovitskiy2021imageworth16x16words">(Dosovitskiy et al., 2021)</a>. Since the input is a stack of CT scans, it takes the shape \((B, C, H, W)\), where \(B\) is the batch size, \(C\) is the CT scan length, \(H\) is the image height, and \(W\) is the image width. The first thought is to directly apply a 3D CNN, but this would be computationally expensive and memory intensive, especially with high values of \(C\). So, we adopt the 2.5D CNN paradigm depicted below <a class="citation" href="#Avesta2022.11.03.22281923">(Avesta et al., 2022)</a>, in which we process the CT scans in separate slices and concatenating the extracted features.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rsna_atd/2.5d-3d-480.webp 480w,/assets/img/rsna_atd/2.5d-3d-800.webp 800w,/assets/img/rsna_atd/2.5d-3d-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rsna_atd/2.5d-3d.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 2.5D vs. 3D convolutional neural network <a class="citation" href="#Avesta2022.11.03.22281923">(Avesta et al., 2022)</a>. </div> <p>We define a preset slice length \(L\) that represents the number of channels each of these processed slices consist of. Thus, we can process this as follows, where <code>SLICE_CHANNELS</code> = \(L\):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">scans</span><span class="p">.</span><span class="n">shape</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">scans</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="p">(</span><span class="n">c</span> <span class="o">//</span> <span class="n">SLICE_CHANNELS</span><span class="p">),</span> <span class="n">SLICE_CHANNELS</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">//</span> <span class="n">SLICE_CHANNELS</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>The backbone can be defined with native PyTorch model definitions. Note that (1) the first convolutional layer must be changed to reflect the chosen slice length and (2) the network head is discarded.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">ResNet18_Weights</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="nf">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">backbone</span><span class="p">.</span><span class="nf">children</span><span class="p">())[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]))</span>
<span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">SLICE_CHANNELS</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <h3 id="mask-generator">Mask Generator</h3> <p>The idea for the mask generator component of the pipeline is to predict a mask region for relevant organs to provide the model more context in completing the downstream task of classifying injuries. Both SAM-Med2D and TotalSegmentator were investigated for this mask generation task.</p> <h4 id="sam-med2d">SAM-Med2D</h4> <p>This model is a fine-tuned version on the Segment Anything Model and trained on 4.6 million medical images. SAM-Med2D incorporates domain-specific knowledge from the medical field by adapting adapter layers within the base transformer blocks <a class="citation" href="#cheng2023sammed2d">(Cheng et al., 2023)</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rsna_atd/sam-med2d-480.webp 480w,/assets/img/rsna_atd/sam-med2d-800.webp 800w,/assets/img/rsna_atd/sam-med2d-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rsna_atd/sam-med2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> SAM-Med2D model <a class="citation" href="#cheng2023sammed2d">(Cheng et al., 2023)</a>. </div> <p>Following the instructions from the <a href="https://github.com/OpenGVLab/SAM-Med2D" rel="external nofollow noopener" target="_blank">official implementation</a>, we can generate and apply the masks as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">SAM_Med2D.segment_anything</span> <span class="kn">import</span> <span class="n">sam_model_registry</span>
<span class="kn">from</span> <span class="n">SAM_Med2D.segment_anything.automatic_mask_generator</span> <span class="kn">import</span> <span class="n">SamAutomaticMaskGenerator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sam_model_registry</span><span class="p">[</span><span class="sh">"</span><span class="s">vit_b</span><span class="sh">"</span><span class="p">](</span><span class="nc">Namespace</span><span class="p">(</span><span class="n">image_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">encoder_adapter</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sam_checkpoint</span><span class="o">=</span><span class="n">MASK_MODEL</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">mask_generator</span> <span class="o">=</span> <span class="nc">SamAutomaticMaskGenerator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pred_iou_thresh</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">stability_score_thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_masks</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">MASK_DEPTH</span> <span class="c1"># 12
</span>    <span class="k">if</span> <span class="nb">id</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.npz</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">MASK_FOLDER</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">mode</span><span class="p">)):</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">MASK_FOLDER</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">mode</span><span class="p">,</span> <span class="nb">id</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.npz</span><span class="sh">'</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">N_CHANNELS</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
            <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="p">(</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span><span class="n">i</span> <span class="o">+</span> <span class="p">(</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">masks</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">save_masks</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">N_CHANNELS</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
            <span class="n">image</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:].</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mask_generator</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">))</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">masks</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">logical_and</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">segmentation</span><span class="sh">'</span><span class="p">],</span> <span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">stability_score</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">mask</span><span class="p">),</span> <span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">stability_score</span><span class="sh">'</span><span class="p">],</span> <span class="n">mask</span><span class="p">)</span>
            <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="p">(</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span><span class="n">i</span> <span class="o">+</span> <span class="p">(</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">mask</span>
            <span class="n">save_masks</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">mask</span>
        <span class="n">np</span><span class="p">.</span><span class="nf">savez</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">MASK_FOLDER</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">mode</span><span class="p">,</span> <span class="nb">id</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.npz</span><span class="sh">'</span><span class="p">),</span> <span class="o">**</span><span class="n">save_masks</span><span class="p">)</span>
</code></pre></div></div> <p>In hindsight, I shouldn’t have chosen <code class="language-plaintext highlighter-rouge">id</code> and <code class="language-plaintext highlighter-rouge">input</code> as variable names. <img class="emoji" title=":laughing:" alt=":laughing:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png" height="20" width="20"></p> <p>The naive approach is to generate the mask and zero out the CT-scan input at non-masked locations. This new masked input can then be feature extracted and concatenated with the features from the original input. This feature vector can then be fused with a multi-layer perceptron. However, another approach would be to implement an attention-based mechanism, which is likely a better utilization of the organ segmentation context. This can be done with a scaled dot product cross attention, where keys \(K\) and values \(V\) are derived from the original input and queries \(Q\) are computed from the segmentation mask.</p> <h4 id="totalsegmentator">TotalSegmentator</h4> <p>Because SAM-Med2D falls under the segment anything framework, it is difficult to accurately obtain specific organ segmentations. Instead, we look at TotalSegmentator. This approach is, for the most part, accurate, but suffers from computational inefficiency.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="cheng2023sammed2d" class="col-sm-8"> <div class="title">SAM-Med2D</div> <div class="author"> Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen , and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, Yu Qiao' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> Jul 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Avesta2022.11.03.22281923" class="col-sm-8"> <div class="title">Comparing 3D, 2.5D, and 2D Approaches to Brain Image Segmentation</div> <div class="author"> Arman Avesta, Sajid Hossain, MingDe Lin, Mariam Aboian , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Harlan M. Krumholz, Sanjay Aneja' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>medRxiv</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Deep-learning methods for auto-segmenting brain images either segment one slice of the image (2D), five consecutive slices of the image (2.5D), or an entire volume of the image (3D). Whether one approach is superior for auto-segmenting brain images is not known.We compared these three approaches (3D, 2.5D, and 2D) across three auto-segmentation models (capsule networks, UNets, and nnUNets) to segment brain structures. We used 3430 brain MRIs, acquired in a multi-institutional study, to train and test our models. We used the following performance metrics: segmentation accuracy, performance with limited training data, required computational memory, and computational speed during training and deployment.3D, 2.5D, and 2D approaches respectively gave the highest to lowest Dice scores across all models. 3D models maintained higher Dice scores when the training set size was decreased from 3199 MRIs down to 60 MRIs. 3D models converged 20% to 40% faster during training and were 30% to 50% faster during deployment. However, 3D models require 20 times more computational memory compared to 2.5D or 2D models.This study showed that 3D models are more accurate, maintain better performance with limited training data, and are faster to train and deploy. However, 3D models require more computational memory compared to 2.5D or 2D models.Competing Interest StatementThe authors have declared no competing interest.Funding StatementArman Avesta is a PhD Student in the Investigative Medicine Program at Yale which is supported by CTSA Grant Number UL1 TR001863 from the National Center for Advancing Translational Science a component of the National Institutes of Health (NIH). This work was also supported by the Radiological Society of North America (RSNA) Fellow Research Grant Number RF2212. The contents of this article are solely the responsibility of the authors and do not necessarily represent the official views of NIH or RSNA.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:the data used in this study were obtained from the Alzheimer Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). We obtained T1-weighted MRIs of 3430 patients in the Alzheimer Disease Neuroimaging Initiative study from this data-sharing platform. The investigators within the ADNI contributed to the design and implementation of ADNI but did not participate in the analysis or writing of this article.I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.Yesthe data used in this study were obtained from the Alzheimer Disease Neuroimaging Initiative (ADNI) database available at: adni.loni.usc.edu. https://adni.loni.usc.edu/ 2D segmentationtwo-dimensional segmentation2.5D segmentationenhanced two-dimensional segmentation3D segmentationthree-dimensional segmentationADNIAlzheimer’s disease neuroimaging initiativeCapsNetcapsule networkCPUcentral processing unitCTcomputed tomographyGBgiga-byteGPUgraphics processing unitMRImagnetic resonance imaging</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="dosovitskiy2021imageworth16x16words" class="col-sm-8"> <div class="title">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</div> <div class="author"> Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> Jul 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="he2015deepresiduallearningimage" class="col-sm-8"> <div class="title">Deep Residual Learning for Image Recognition</div> <div class="author"> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun </div> <div class="periodical"> Jul 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Joe Lin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/libs/mathjax/tex-mml-chtml.min.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/polyfill/polyfill.min.js" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"recent research works",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"a collection of academic and extracurricular projects in machine learning and web development!",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-notes",title:"notes",description:"collection of detailed notes and summaries from my academic courses",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-digital-humans",title:"digital humans",description:"a deep dive into representing and understanding humans",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-humans/"}},{id:"news-lt-strong-gt-learning-to-generate-diverse-pedestrian-movements-from-web-videos-with-noisy-labels-lt-strong-gt-is-accepted-to-lt-em-gt-iclr-2025-lt-em-gt",title:"&lt;strong&gt;Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels&lt;/strong&gt; is accepted to &lt;em&gt;ICLR 2025&lt;/em&gt;.",description:"",section:"News"},{id:"news-lt-strong-gt-learning-assistant-lt-strong-gt-for-prof-bolei-zhou-s-cs-163",title:"&lt;strong&gt;Learning Assistant&lt;/strong&gt; for Prof. Bolei Zhou\u2019s CS 163!",description:"",section:"News"},{id:"news-joined-zhou-lab-at-ucla",title:"Joined Zhou Lab at UCLA.",description:"",section:"News"},{id:"notes-cs180-introduction-to-algorithms-and-complexity",title:"cs180 - introduction to algorithms and complexity",description:"course notes from summer &#39;24",section:"Notes",handler:()=>{window.location.href="/notes/cs180/"}},{id:"notes-cs188-deep-learning-for-computer-vision",title:"cs188 - deep learning for computer vision",description:"course notes from winter &#39;24",section:"Notes",handler:()=>{window.location.href="/notes/cs188/"}},{id:"projects-cifar-image-classification",title:"cifar image classification",description:"using popular deep learning architectures",section:"Projects",handler:()=>{window.location.href="/projects/cifar/"}},{id:"projects-autonomous-vehicle-expo",title:"autonomous vehicle expo",description:"eagle project conference website",section:"Projects",handler:()=>{window.location.href="/projects/conference/"}},{id:"projects-novel-view-synthesis",title:"novel view synthesis",description:"com sci 188 final project (w/ michael song and alexander chien)",section:"Projects",handler:()=>{window.location.href="/projects/novel_view_synthesis/"}},{id:"projects-abdominal-trauma-detection",title:"abdominal trauma detection",description:"rsna kaggle competition",section:"Projects",handler:()=>{window.location.href="/projects/rsna_atd/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%6F%65%6C%69%6E%74%65%63%68@%75%63%6C%61.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/joe-lin-tech","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/joe-lin-tech","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/16385395","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>
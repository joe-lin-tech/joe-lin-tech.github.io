<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> cifar image classification | Joe Lin </title> <meta name="author" content="Joe Lin"> <meta name="description" content="using popular deep learning architectures"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joe-lin-tech.github.io/projects/cifar/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joe</span> Lin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">cifar image classification</h1> <p class="post-description">using popular deep learning architectures</p> </header> <article> <blockquote> <p>Classify images from the CIFAR-10 dataset using a variety of modern architectures.</p> </blockquote> <h2 id="project-overview">Project Overview</h2> <p>This project implements a training and testing pipeline for an image classification task on the CIFAR-10 dataset. CIFAR-10 contains 60,000 32x32 RGB images distributed evenly across 10 image classes (6,000 images per class). The provided dataset splits consists of a train set with 50,000 images and a test set with 10,000 images. Here, the train set is further split into a train set with 45,000 images and a validation set with 5,000 images to allow for model evaluation throughout the training process. The models implemented in this repository includes a basic CNN, a resnet, and a vision transformer.</p> <h2 id="setup-and-run">Setup and Run</h2> <p>The repository contains both a python script and a Jupyter notebook. Each of their setup/run procedures are detailed below.</p> <h3 id="python-script">Python Script</h3> <p>Clone the repository.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:joe-lin-tech/cifar.git
<span class="nb">cd </span>cifar
</code></pre></div></div> <p>Create and activate a virtual environment. (Alternatively, use an existing environment of your choosing.)</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv venv
<span class="nb">source </span>venv/bin/activate
</code></pre></div></div> <p>Install required pip packages and dependencies.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>Login to a wandb account if you’d like to view train logs. (If not, make sure to toggle respective flag when running.)</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wandb login
</code></pre></div></div> <p>Your local environment should now be suitable to run the main script <code class="language-plaintext highlighter-rouge">train.py</code>. You can either run it interactively or use the shell to specify run options.</p> <h4 id="run-interactively">Run Interactively</h4> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 train.py
</code></pre></div></div> <h4 id="run-in-the-shell">Run in the Shell</h4> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 train.py <span class="nt">-m</span> previt <span class="nt">-d</span> cuda
</code></pre></div></div> <p>The above command fine tunes a vision transformer pretrained on ImageNet with hyperparameters set to those used in this project. For reproducibility tests, specifying <code class="language-plaintext highlighter-rouge">shell -m</code> and <code class="language-plaintext highlighter-rouge">shell -d</code> like above will be sufficient. Additional specifiers detailed below.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 train.py <span class="nt">-m</span> resnet <span class="nt">-e</span> 50 <span class="nt">-b</span> 128 <span class="nt">-l</span> 0.1 <span class="nt">-d</span> cuda
</code></pre></div></div> <p>As an example of a more customized run, the above command trains a resnet-based model on cuda for 50 epochs with batch size of 128 and initial learning rate of 0.1.</p> <table id="table" data-toggle="table" class="mb-3" style="width: 100%"> <thead> <tr> <th data-field="specifier">Specifier</th> <th data-field="usage">Usage</th> </tr> </thead> <tbody> <tr> <td> <code>-m</code>, <code>--model</code> </td> <td>choose model architecture (<code>cnn</code>, <code>resnet</code>, <code>previt</code>, or <code>vit</code>)</td> </tr> <tr> <td> <code>-e</code>, <code>--epoch</code> </td> <td>number of epochs</td> </tr> <tr> <td> <code>-b</code>, <code>--batch-size</code> </td> <td>batch size</td> </tr> <tr> <td> <code>-l</code>, <code>--learning-rate</code> </td> <td>learning rate</td> </tr> <tr> <td> <code>-d</code>, <code>--device</code> </td> <td>device</td> </tr> <tr> <td> <code>-c</code>, <code>--cross-validate</code> </td> <td>flag for training with 5-fold cross-validation (default: False)</td> </tr> <tr> <td> <code>-w</code>, <code>--wandb</code> </td> <td>flag for wandb logging (default: False)</td> </tr> <tr> <td> <code>-s</code>, <code>--save-folder</code> </td> <td>path to desired model save folder (default: current working directory)</td> </tr> <tr> <td> <code>-f</code>, <code>--ckpt-frequency</code> </td> <td>how often to save model checkpoint, in number of epochs (default: 0, save final)</td> </tr> </tbody> </table> <h3 id="jupyter-notebook">Jupyter Notebook</h3> <p>Download the Jupyter notebook and run the first cell to import relevant packages. The following Python packages are used for this project and may need to be installed directly (if not installed in current environment) with <code class="language-plaintext highlighter-rouge">!pip install &lt;package name&gt;</code>.</p> <ul> <li> <strong>General Purpose:</strong> For shuffling and seeding random processes, use <code class="language-plaintext highlighter-rouge">random</code>. To read and write to local file system, use <code class="language-plaintext highlighter-rouge">os</code>.</li> <li> <strong>Data Manipulation:</strong> Use <code class="language-plaintext highlighter-rouge">numpy</code> to represent and manipulate data.</li> <li> <strong>Machine Learning:</strong> Use <code class="language-plaintext highlighter-rouge">torch</code> and <code class="language-plaintext highlighter-rouge">torchvision</code>, which are suitable for Computer Vision tasks. For logging the training loop, use <code class="language-plaintext highlighter-rouge">wandb</code>.</li> </ul> <p>Run the remaining cells to execute the training procedure of the latest notebook version (pretrained vision transformer).</p> <h2 id="model-architecture-and-training">Model Architecture and Training</h2> <h3 id="basic-cnn-architecture">Basic CNN Architecture</h3> <p>This implementation consists of 3 convolutional layers (conv + relu + max pool) and a fully connected network.</p> <table id="table" data-toggle="table" class="mb-3" style="width: 100%"> <thead> <tr> <th data-field="layer">Layer</th> <th data-field="parameters">Parameters</th> </tr> </thead> <tbody> <tr> <td><code>nn.Conv2d</code></td> <td> <code>in_channels</code> = 3, <code>out_channels</code> = 8, <code>kernel_size</code> = 5, <code>stride</code> = 1, <code>padding</code> = 2</td> </tr> <tr> <td><code>nn.MaxPool2d</code></td> <td> <code>kernel_size</code> = 2, <code>stride</code> = 2</td> </tr> <tr> <td><code>nn.Conv2d</code></td> <td> <code>in_channels</code> = 8, <code>out_channels</code> = 16, <code>kernel_size</code> = 5, <code>stride</code> = 1, <code>padding</code> = 2</td> </tr> <tr> <td><code>nn.MaxPool2d</code></td> <td> <code>kernel_size</code> = 2, <code>stride</code> = 2</td> </tr> <tr> <td><code>nn.Conv2d</code></td> <td> <code>in_channels</code> = 16, <code>out_channels</code> = 32, <code>kernel_size</code> = 5, <code>stride</code> = 1, <code>padding</code> = 2</td> </tr> <tr> <td><code>nn.MaxPool2d</code></td> <td> <code>kernel_size</code> = 2, <code>stride</code> = 2</td> </tr> <tr> <td><code>nn.Linear</code></td> <td> <code>in_channels</code> = 512, <code>out_channels</code> = 64</td> </tr> <tr> <td><code>nn.Linear</code></td> <td> <code>in_channels</code> = 64, <code>out_channels</code> = 32</td> </tr> <tr> <td><code>nn.Linear</code></td> <td> <code>in_channels</code> = 32, <code>out_channels</code> = 10</td> </tr> </tbody> </table> <p>Using the hyperparameters below, the model is capable of achieving ~50% test accuracy on CIFAR-10.</p> <table id="table" data-toggle="table" class="mb-3" style="width: 20%"> <thead> <tr> <th data-field="hyperparameter">Hyperparameter</th> <th data-field="value">Value</th> </tr> </thead> <tbody> <tr> <td>EPOCHS</td> <td>20</td> </tr> <tr> <td>BATCH_SIZE</td> <td>128</td> </tr> <tr> <td>LEARNING_RATE</td> <td>1e-4</td> </tr> </tbody> </table> <table id="table" data-toggle="table" class="mb-3" style="width: 40%"> <thead> <tr> <th data-field="optimizer">Optimizer</th> <th data-field="parameters">Parameters</th> </tr> </thead> <tbody> <tr> <td>Adam</td> <td> <code>weight_decay</code> = 0.01</td> </tr> </tbody> </table> <p>Below is the wandb log of training the basic CNN model:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cifar/cnn-480.webp 480w,/assets/img/cifar/cnn-800.webp 800w,/assets/img/cifar/cnn-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cifar/cnn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 1. wandb logs from basic cnn training. </div> <h3 id="resnet-architecture">ResNet Architecture</h3> <p>This implementation utilizes residual connections to improve learning and allow us to build a deeper neural network, all whilst maintaining gradient flow. The original ResNet paper was referred to for implementation and technical details <a class="citation" href="#he2015deepresiduallearningimage">(He et al., 2015)</a>.</p> <p>Using the hyperparameters below, the model is capable of achieving ~91% test accuracy on CIFAR-10.</p> <table id="table" data-toggle="table" class="mb-3" style="width: 20%"> <thead> <tr> <th data-field="hyperparameter">Hyperparameter</th> <th data-field="value">Value</th> </tr> </thead> <tbody> <tr> <td>EPOCHS</td> <td>50</td> </tr> <tr> <td>BATCH_SIZE</td> <td>128</td> </tr> <tr> <td>LEARNING_RATE</td> <td>0.1</td> </tr> </tbody> </table> <table id="table" data-toggle="table" class="mb-3" style="width: 70%"> <thead> <tr> <th data-field="optimizer">Optimizer</th> <th data-field="parameters">Parameters</th> </tr> </thead> <tbody> <tr> <td>SGD</td> <td> <code>momentum</code> = 0.9, <code>weight_decay</code> = 5e-4, <code>nesterov</code> = True</td> </tr> </tbody> </table> <table id="table" data-toggle="table" class="mb-3" style="width: 80%"> <thead> <tr> <th data-field="scheduler">Scheduler</th> <th data-field="parameters">Parameters</th> </tr> </thead> <tbody> <tr> <td>ReduceLROnPlateau</td> <td> <code>mode</code> = max, <code>factor</code> = 0.1, <code>patience</code> = 3, <code>threshold</code> = 1e-3</td> </tr> </tbody> </table> <p>Below is the wandb log of training the ResNet model:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cifar/resnet-480.webp 480w,/assets/img/cifar/resnet-800.webp 800w,/assets/img/cifar/resnet-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cifar/resnet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 1. wandb logs from resnet training. </div> <h3 id="vision-transformer">Vision Transformer</h3> <p>The final implementation harnesses the expressive capabilities of transformers, especially with its utilization of self-attention <a class="citation" href="#dosovitskiy2021imageworth16x16words">(Dosovitskiy et al., 2021)</a>. Note that instead of patchifying the image and linear projecting, a convolutional layer is applied to obtain patch embeddings. This modification helps “increase optimization stability and also improves peak performance” as described in <a class="citation" href="#xiao2021earlyconvolutionshelptransformers">(Xiao et al., 2021)</a>.</p> <p>This project consists of both (1) fine-tuning a vision transformer pretrained on ImageNet and (2) training a vision transformer from scratch.</p> <p>Using the hyperparameters below, the pretrained vision transformer can be fine tuned to achieve ~97.6% test accuracy (cross-validated) on CIFAR-10.</p> <table id="table" data-toggle="table" class="mb-3" style="width: 20%"> <thead> <tr> <th data-field="hyperparameter">Hyperparameter</th> <th data-field="value">Value</th> </tr> </thead> <tbody> <tr> <td>EPOCHS</td> <td>10</td> </tr> <tr> <td>BATCH_SIZE</td> <td>32</td> </tr> <tr> <td>LEARNING_RATE</td> <td>1e-4</td> </tr> </tbody> </table> <table id="table" data-toggle="table" class="mb-3" style="width: 60%"> <thead> <tr> <th data-field="optimizer">Optimizer</th> <th data-field="parameters">Parameters</th> </tr> </thead> <tbody> <tr> <td>Adam</td> <td> <code>momentum</code> = 0.9, <code>weight_decay</code> = 1e-7</td> </tr> </tbody> </table> <table id="table" data-toggle="table" class="mb-3" style="width: 30%"> <thead> <tr> <th data-field="scheduler">Scheduler</th> <th data-field="parameters">Parameters</th> </tr> </thead> <tbody> <tr> <td>CosineAnnealingLR</td> <td> <code>T_max</code> = 10</td> </tr> </tbody> </table> <p>The same hyperparameters are used to train a vision transformer from scratch except the learning rate is reduced to 1e-5, a different learning rate scheduler was used, and longer training time (details to be added soon).</p> <p>Below is the wandb log of losses and learning rate for both of these training sessions (fine tune and from scratch):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cifar/vit-480.webp 480w,/assets/img/cifar/vit-800.webp 800w,/assets/img/cifar/vit-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cifar/vit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 1. wandb logs from vision transformer training. </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="dosovitskiy2021imageworth16x16words" class="col-sm-8"> <div class="title">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</div> <div class="author"> Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov , and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="xiao2021earlyconvolutionshelptransformers" class="col-sm-8"> <div class="title">Early Convolutions Help Transformers See Better</div> <div class="author"> Tete Xiao, Mannat Singh, Eric Mintun , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Trevor Darrell, Piotr Dollár, Ross Girshick' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="he2015deepresiduallearningimage" class="col-sm-8"> <div class="title">Deep Residual Learning for Image Recognition</div> <div class="author"> Kaiming He, Xiangyu Zhang, Shaoqing Ren , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jian Sun' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Joe Lin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/libs/mathjax/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="/assets/libs/polyfill/polyfill.min.js" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"coming soon!",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"a collection of academic and extracurricular projects in machine learning and web development!",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-digital-humans",title:"digital humans",description:"(draft) a deep dive into representing and understanding humans",section:"Posts",handler:()=>{window.location.href="/blog/2024/digital-humans/"}},{id:"news-joined-zhou-lab-at-ucla",title:"Joined Zhou Lab at UCLA.",description:"",section:"News"},{id:"notes-cs180-introduction-to-algorithms-and-complexity",title:"cs180 - introduction to algorithms and complexity",description:"course notes from summer &#39;24",section:"Notes",handler:()=>{window.location.href="/notes/cs180/"}},{id:"projects-cifar-image-classification",title:"cifar image classification",description:"using popular deep learning architectures",section:"Projects",handler:()=>{window.location.href="/projects/cifar/"}},{id:"projects-autonomous-vehicle-expo",title:"autonomous vehicle expo",description:"eagle project conference website",section:"Projects",handler:()=>{window.location.href="/projects/conference/"}},{id:"projects-novel-view-synthesis",title:"novel view synthesis",description:"com sci 188 final project (w/ michael song and alexander chien)",section:"Projects",handler:()=>{window.location.href="/projects/novel_view_synthesis/"}},{id:"projects-abdominal-trauma-detection",title:"abdominal trauma detection",description:"rsna kaggle competition",section:"Projects",handler:()=>{window.location.href="/projects/rsna_atd/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%6F%65%6C%69%6E%74%65%63%68@%75%63%6C%61.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/joe-lin-tech","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/joe-lin-tech","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/16385395","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>